---
layout: post
date: 2021-08-26 14:00
title: LogBERT Log Anomaly Detection via BERT
author: Haixuan Guo, Shuhan Yuan, and Xintao Wu
description: LogBERT paper
comments: true
math: true
category: 
- AI
tags:
- Log
- Anomaly Detection
- Paper
---

Log Anomaly Detection Taks에 BERT(2layer) 적용, 2가지 Self-supervised learning task 제안
 <!--more-->

## Abstract
> Detecting anomalous events in online computer systems is crucial to protect the systems from malicious attacks or malfunctions. System logs, which record detailed information of computational events, are widely used for system status analysis. In this paper, we propose LogBERT, a self-supervised framework for log anomaly detection based on Bidirectional Encoder Representations from Transformers (BERT). **LogBERT learns the patterns of normal log sequences by two novel selfsupervised training tasks and is able to detect anomalies where the underlying patterns deviate from normal log sequences.** The experimental results on three log datasets show that LogBERT outperforms state-ofthe- art approaches for anomaly detection.

##  Introduction
- **System logs**, which record detailed information about computational events generated by computer systems, **play an important role in anomaly detection** nowadays.
- Due to the **data imbalance issue**, <u>it is infeasible to train a binary classifier to detect anomalous log sequences</u>. As a result, many unsupervised learning models, such as **Principal Component Analysis (PCA)**, or one class classification models, such as **one-class SVM**, are widely-used to detect anomalies.
- However, traditional machine learning models, such as one-class SVM, <u>are hard to capture the temporal information of discrete log messages.</u>
- 다른 NLP 영역에서 처럼 BERT를 쓴 이유는 간단. Bidirectional Contextual Encoding 이 필요.
- we propose two self-supervised training tasks: 
  - 1) **masked log key prediction**, which aims to correctly predict log keys in normal log sequences that are randomly masked; 
  - 2) **volume of hypersphere minimization**, which aims to make the normal log sequences close to each other in the embedding space.

##  Related Work
- The tradi-tional approaches explicitly use the keywords (e.g., "fail") or regular expressions to detect anomalous log entries.
  - Sequence Attack은 예측 못함. 키워드 탐색만 가능
  - Predefined Rule에 따른 접근법 --> heavy manual engineering.
- The typical pipeline for Learning Approach
  - First, **a log parser** is adopted to transform log messages to log keys. (log parser가 뭐야?)
  - Second, A feature extraction approach, such as **TF-IDF**, is then used to build a feature vector to represent a sequence of log keys in a sliding window.
  - Third, **unsupervised approach** is applied for detecting the anomalous sequences
- 이전 딥러닝 approach는 RNN/GRU 위주였으나 우리는 BERT를 쓰겠다.

## LogBERT 
- ![archi](/assets/img/logbert1.png)
### Framework
- We aim to detect whether this sequence is normal or anomalous.
- In order to represent log messages, following a typical pre-processing approach, <u>we first extract <strong>log keys</strong>(string templates) from log messages via a log parser (shown in Figure 2).</u>
- ![logkey](/assets/img/logbert2.png)
- bert로 하여금 어떤 sequence가 Noraml Sequence Set에 포함되는지를 prediction 함으로써, **derive an anomaly detection criterion** to identify anomalous sequences.
- **Input Representation** : \\(S^j=[DIST,k^j_1, ...,k^j_t, ..., k^j_T]\\), 
  - <span class='my_highlight'> 'DIST' represents the whole log sequence based on the structure of Transformer encoder.</span>
  - log key는 랜덤한 embedding matraix를 만들어서 임베딩함
  - position encoding 은 sinusoid function 사용
  - Transformer Encoder는 그냥 원래랑 똑같은 그것.

### Objective Function
- **Task 1: Masked Log Key Prediction (MLKP)** 
  - MLM하고 똑같은데, Vocab이 이 친구들은 Log Key Set이다. 나머지는 똑같은듯
  - ![mlm](/assets/img/logbert3.png)
  - we expect once LogBERT is able to correctly predict the masked log keys, it can distinguish the normal and anomalous log sequences.
- **Task II: Volume of Hypersphere Minimization (VHM)**
  - Inspired Deep SVDD approach( SVM의 Shphere 버전??)
  > ![svdd](/assets/img/svdd.jfif)
  - normal log sequences 끼리 embedding space에서 거리는 가깝고, anomalous log sequences는 멀어지도록 하자!
  - Train Set 내의 모든 정상 샘플들의 DIST 토큰의 Encoding 값의 평균(C)을 구하고, 각각의 DIST 토큰들은 그 평균에 가까워 지도록 해보자!
  > 그럼 그 평균 C는 계속 바뀌겠네? Train 내내.. 그리고 batch 단위의 계산이 아니고 모든 sample을 다 구하는건가?
  - ![vhm](/assets/img/logbert4.png)
  - we expect all the normal log sequences in the training set are close to the center, while the anomalous log sequences have a larger distance to the center
  > C로 뭉치게 훈련하면, MLM에도 도움이 될거라는데.. 흠.. 잘 모르겠다
  - 최종 loss ![vhm](/assets/img/logbert5.png)

## Experiments
### Dataset
- ![dataset](/assets/img/logbert6.png)
- HDFS : Hadoop-based map-reduce jobs on Amazon EC2, 평균 길이는 **19**, 휴먼 레이블
- BlueGene/L Supercomputer System (BGL) : collected from a BlueGene/L supercomputer system at Lawrence Livermore National Labs (LLNL). alert vs non-alert. 5분 단위로 sliding window 해가면서 데이터 만듬. 평균 길이는 **562**
- Thunderbird: supercomputer, 1 min sliding, 펴균 길이 **326**
### Baseline
- PCA : count 기반 matrix를 저차원으로 줄임
- One-Class SVM : one-class classification model
- IsolationForest : Tree structure Unsupervised learning
- LogCluster : Clustering based approach
- DeepLog : RNN 사용해서, normal에 훈련하고, anomaly에 적용하는듯.
- LogAnomaly : DeepLearning Based approach. 설명이 적음

### Implementation
- **Drain**을 이용해서 log key를 뽑아냄.
- **Transformer Layer는 단 2개**
- input representation 50, hidden 256
- total process
- ![total](/assets/img/logbert7.png)
- code : https://github.com/HelenGuohx/logbert

### Anomaly Detection
- 재밌는 방식으로 Detection을 하네.
- Test Set에서 Randomly Masked 한 다음, LogBERT에 넣고 Masked Token을 예측하는데, Top g개를 뽑고 그게 실제 Token이면 정상, 아니면 abnormal. 몇개 이상? r 개 이상이 anomaly면 anomaly로 함. r, g는 하이퍼 파람

### Results
- ![result](/assets/img/logbert8.png)
  - Precision : 모델이 맞춘 정답중, error를 맞춘 비율
  - Recall : 실제 error인 샘플 중, 모델이 error라고 한 비율(모두다 에러라고 말하면 100%)
- 결과를 보면, PCA와 OCSVM이 성능이 구림. temporal information에 대한 학습이 안된것으로 파악됨.
- logCluster가 다른 ml 방식보다 좋음
- 다른 딥러닝 모델들은 f1에서 LogBERT를 이기지 못함

### Ablation Studies
- ![aba](/assets/img/logbert9.png)
- 2개의 task중 1개씩 해보고 결과를 재봄.
- VHM만 할때는 그냥 DIST와 normal sample의 평균인 C와의 거리가 threshold보다 길면 abnormal로 판단
- HDFS외에 다른 두개의 셋은 VHM이 크게 도움이 안됬는데, 이는 HDFS의 길이가 19로 매우 적기 때문

### Visualization
- DIST 토큰의인코딩 값을 이용해서 2dimension으로 표현해봄. 정상 비정상 각각 1000개씩 from HDFS
  - ![vi](/assets/img/logbert10.png)
  - locally linear embedding (LLE) algorithm을 사용
> 이게 뭐지?
- BGL에 대한 하이퍼 파라미터 분석 결과
  - ![vi](/assets/img/logbert11.png)
  - 좀 의외로 알파값이 거의 영향이 없음. 그 이유를 BGL은 길이가 길어서 MLM Loss가 너무 지배적이기 때문이라고 분석.

## Conclusion
- BERT와 2가지 self-supervised training tasks를 이용해서 log anomaly detection에 적용하여, Sota 찍음.