---
layout: post
date: 2022-05-11 00:00
created_date: 2022-05-11 00:00
title: "[Paper] CM3: A Causal Masked Multimodal Model of the Internet"
author: oglee
description: A Causal Masked Multimodal Model of the Internet
comments: true
math: true
category:
- Paper
tags:
- NLP
- Prompt Learning
- HTML
- CM3
- Multi-modal
---

HTML 코드를 그대로 활용하여 Multimodal learning 하는 페이퍼인 CM3 정리
 <!--more-->
 
 # Abstract
 _We introduce **CM3**, a family of **causally masked generative models** trained over a large corpus of **structured multi-modal documents** that can contain **both text and image tokens.**
 Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated atthe end of the string, instead of their original positions. 
 The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. 
 We train causally masked languageimage models on **large-scale web and Wikipedia articles**, where each document contains all of the text, **hypertext markup**, hyperlinks, and image tokens (from a **VQVAE-GAN**), provided in the order they appear in the original HTML source (before masking). 
 The resulting CM3 models can generate rich structured, multimodal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. 
 They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM (Ramesh et al., 2021; De Cao et al., 2020; Aghajanyan et al., 2021). 
 We set the **new state-of-the-art in zero-shot summarization**, entity linking, and entity disambiguation while maintaining competitiveperformance in the fine-tuning setting. 
 **We can generate images unconditionally**, conditioned on text (**like DALL-E**) and do captioning all in a zero-shot setting with a single model._
 
 # Introduction
- We present the first **hyper-text language-image model**, trained on close to a Terabyte of
multi-modal simplified HTML data from the common crawl.
- We present the causally masked objective, a **hybrid of causal and masked language models**
that allows for bidirectional context control during generative mask infilling.
- We demonstrate consistently strong transfer from CM3 to a range of uni-modal and multimodal tasks at differing supervision levels, including stating **state-of-the-art on entity disambiguation and zero-shot summarization.**

# Causally Masked Language Model?
|  | Models | Pros | Cons |
|---|---|---|---|
| Masked Language Models | BERT, RoBERTa | Bi-directional Context | Non-generative, decoding only 15% |
| Causally Language Models | GPTs | Generative, per token generation | Uni-directional |
| Causally Masked Language Models | CM3 | Bi-directional context + per token generation | 굳이 말하자면 학습 속도..? |

- Causally Masked Language Modeling : combined the benefit of **per-token generation** with optional **bi-directionality** specifically **tailored to prompting.**
- ![image](https://user-images.githubusercontent.com/18374514/167800457-c09372f0-9f46-4903-9342-58e0405f8285.png)
- Number of Mask : N~Clamp(Poisson(1), 1, 16)
 - ![image](https://user-images.githubusercontent.com/18374514/167794104-c082bf94-9c9a-49fe-b4f8-6497b80309f5.png)
- Mask Span Length : Uni(0, doc_length)
- **Mask는 적게, Span을 길게 만들어서 상대적으로 long span을 generation하는 능력을 배울 수 있도록 유도**
- 각 Mask는 등장 순서대로 번호를 매기며, 맨 뒤에도 순서대로 추가함(위 그림 참조)
- 이제 GPT와 같은 left-to-right 방식의 generative 한 방식으로 학습을 진행
  - 그러나 맨 뒤에 추가한 MASK 토큰 이후로는 중간에 가려진 Context를 유추해야하기 때문에 Bi-directional context를 이용하는 방식을 배울 수 있음.
  - `기발한 발상 같다. 기존 BERT의 MLM 방식에서는 New york 같이 서로 correlation이 깊은 Text의 경우, 모두 MASK 되버리면 서로 상호 관계를 배울 수가 없는 한계가 있었는데, 이 페이퍼에서 제안하는 방식대로 하면 left-to-right 방식으로 mask를 generation하기 때문에 이와 같은 문제도 어느정도 해결 할 수 있지 않을까 하는 생각이 든다.`
  - ![image](https://user-images.githubusercontent.com/18374514/167801664-b43d099d-72ad-48df-b083-2a31277ef6fc.png)
- 그리고 **MASK 토큰에 대한 loss는 0으로 만듦**
  - 왜냐하면, 길이가 엄청 긴 S라는 토큰을 MASK 하나로 만들었는데, 이에 대해서 어떤식으로든 예측된 값은 전혀 의미가 없기 때문

# CM3
- 기존 연구[1]가 text only 였다면 본 연구는 multi-modal로 확장시키고 BART-like objective를 버리고 새로운 objective 사용
- 기존연구
  - ![image](https://user-images.githubusercontent.com/18374514/167803099-99f77bff-fe29-4078-a51c-d5411b7ffcc0.png)
 
## Data
- to convert HTML to minimal-html,
  - removed all elements which do not contain textual elements. 
  - filterd out all headers, footers, copyrights, forms, dialog boxes and iFrames. 
  - folded consecutive <div> elements into a singular <div> element with merged attributes. 
  - striped all the attributes from every element which are not derived from structured graphs such as Open Graph, Schema and Twitter
- For every <img> tag in the document with a valid src attribute URL, 
  - we download the image, resize to **256x256** pixels with random cropping and then tokenize it with **VQVAE-GAN** 
  - This amounts to **256 tokens for every image.**
  - We then insert the string value of the tokens joined with a space back into the src attribute.
 
# Reference
- [1] https://arxiv.org/pdf/2107.06955.pdf
