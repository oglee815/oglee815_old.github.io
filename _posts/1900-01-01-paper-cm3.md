---
layout: post
date: 2022-05-11 00:00
created_date: 2022-05-11 00:00
title: "[Paper] CM3: A Causal Masked Multimodal Model of the Internet"
author: oglee
description: A Causal Masked Multimodal Model of the Internet
comments: true
math: true
category:
- Paper
tags:
- NLP
- Prompt Learning
- HTML
- CM3
- Multi-modal
---

HTML 코드를 그대로 활용하여 Multimodal learning 하는 페이퍼인 CM3 정리
 <!--more-->

# Abstract
 _We introduce **CM3**, a family of **causally masked generative models** trained over a large corpus of **structured multi-modal documents** that can contain **both text and image tokens.**
 Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated atthe end of the string, instead of their original positions. 
 The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. 
 We train causally masked languageimage models on **large-scale web and Wikipedia articles**, where each document contains all of the text, **hypertext markup**, hyperlinks, and image tokens (from a **VQVAE-GAN**), provided in the order they appear in the original HTML source (before masking). 
 The resulting CM3 models can generate rich structured, multimodal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. 
 They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM (Ramesh et al., 2021; De Cao et al., 2020; Aghajanyan et al., 2021). 
 We set the **new state-of-the-art in zero-shot summarization**, entity linking, and entity disambiguation while maintaining competitiveperformance in the fine-tuning setting. 
 **We can generate images unconditionally**, conditioned on text (**like DALL-E**) and do captioning all in a zero-shot setting with a single model._
 
# Introduction
- We present the first **hyper-text language-image model**, trained on close to a Terabyte of
multi-modal simplified HTML data from the common crawl.
- We present the causally masked objective, a **hybrid of causal and masked language models**
that allows for bidirectional context control during generative mask infilling.
- We demonstrate consistently strong transfer from CM3 to a range of uni-modal and multimodal tasks at differing supervision levels, including stating **state-of-the-art on entity disambiguation and zero-shot summarization.**

# Causally Masked Language Model?
|  | Models | Pros | Cons |
|---|---|---|---|
| Masked Language Models | BERT, RoBERTa | Bi-directional Context | Non-generative, decoding only 15% |
| Causally Language Models | GPTs | Generative, per token generation | Uni-directional |
| Causally Masked Language Models | CM3 | Bi-directional context + per token generation | 굳이 말하자면 학습 속도..? |

- Causally Masked Language Modeling : combined the benefit of **per-token generation** with optional **bi-directionality** specifically **tailored to prompting.**
- ![image](https://user-images.githubusercontent.com/18374514/167800457-c09372f0-9f46-4903-9342-58e0405f8285.png)
- Number of Mask : N~Clamp(Poisson(1), 1, 16)
 - ![image](https://user-images.githubusercontent.com/18374514/167794104-c082bf94-9c9a-49fe-b4f8-6497b80309f5.png)
- Mask Span Length : Uni(0, doc_length)
- **Mask는 적게, Span을 길게 만들어서 상대적으로 long span을 generation하는 능력을 배울 수 있도록 유도**
- 각 Mask는 등장 순서대로 번호를 매기며, 맨 뒤에도 순서대로 추가함(위 그림 참조)
- 이제 GPT와 같은 left-to-right 방식의 generative 한 방식으로 학습을 진행
  - 그러나 맨 뒤에 추가한 MASK 토큰 이후로는 중간에 가려진 Context를 유추해야하기 때문에 Bi-directional context를 이용하는 방식을 배울 수 있음.
  - `기발한 발상 같다. 기존 BERT의 MLM 방식에서는 New york 같이 서로 correlation이 깊은 Text의 경우, 모두 MASK 되버리면 서로 상호 관계를 배울 수가 없는 한계가 있었는데, 이 페이퍼에서 제안하는 방식대로 하면 left-to-right 방식으로 mask를 generation하기 때문에 이와 같은 문제도 어느정도 해결 할 수 있지 않을까 하는 생각이 든다.`
  - ![image](https://user-images.githubusercontent.com/18374514/167801664-b43d099d-72ad-48df-b083-2a31277ef6fc.png)
- 그리고 **MASK 토큰에 대한 loss는 0으로 만듦**
  - 왜냐하면, 길이가 엄청 긴 S라는 토큰을 MASK 하나로 만들었는데, 이에 대해서 어떤식으로든 예측된 값은 전혀 의미가 없기 때문

# CM3
- 기존 연구[1]가 text only 였다면 본 연구는 multi-modal로 확장시키고 BART-like objective를 버리고 새로운 objective 사용
- 기존 연구인 HTLM 예시
  - ![image](https://user-images.githubusercontent.com/18374514/167803099-99f77bff-fe29-4078-a51c-d5411b7ffcc0.png)
 
## Data
- to convert HTML to minimal-html,
  - removed all elements which do not contain textual elements. 
  - filterd out all headers, footers, copyrights, forms, dialog boxes and iFrames. 
  - folded consecutive <div> elements into a singular <div> element with merged attributes. 
  - striped all the attributes from every element which are not derived from structured graphs such as Open Graph, Schema and Twitter
- For every <img> tag in the document with a valid src attribute URL, 
  - we download the image, resize to **256x256** pixels with random cropping and then tokenize it with **VQVAE-GAN** 
  - This amounts to **256 tokens for every image.**
  - We then insert the string value of the tokens joined with a space back into the src attribute.
- VQVAE-GAN 이미지[2]
  - ![image](https://user-images.githubusercontent.com/18374514/167985911-7ea5072f-4f5b-46dc-921a-138aa9925dba.png)
- 사용된 데이터 statistics
  - ![image](https://user-images.githubusercontent.com/18374514/167987906-6afcb611-984e-434f-bf25-dc7d2361cfab.png)

## Size Hints
- 이전 연구[1]에서 사용된 MASK token의 길이에 대한 힌트를 주는 것은 성능 저하를 불러온다고 판단하여 더이상 사용하지 않음 ( e.g., <mask>12 )

## Training
- 4 models : 125M, 800M, 2.7B, and 13B(Large) parameters.
- Medium : 240 V100 GPU for 28 days, Large : 384 A100 for 24 days
- batch = 384 * 8, maximum token size = 2048
- ![image](https://user-images.githubusercontent.com/18374514/167989303-9f5aa5c1-b24c-4068-8526-69e17631dd6a.png)
 
## Scaling laws
- ![image](https://user-images.githubusercontent.com/18374514/167990868-5e051ed3-b40b-4612-b0c9-902ed769eae1.png)
- There is still a good amount of gains to be achieved with further scaling

## Zero/Few-shot prompting
### Unconditional Image Generation
- Prompt : _<img_
  - 그럼, 모델은 우선 _alt_ 속성에 해당하는 이미지에 대한 설명을 생성하고 그 다음 _src_ 속성에 해당하는 이미지를 생성함
  - ![image](https://user-images.githubusercontent.com/18374514/167992575-a1a12905-d316-44ad-8154-a898273366f2.png)
- Prompt : _<img src="_
  - 그럼 바로 이미지를 생성
  - ![image](https://user-images.githubusercontent.com/18374514/167992611-0ee10b89-9576-4cd1-b5d6-390ba2c2e6e1.png)
- DALL-E 와는 달리, Caption마저 자유롭게 생성하게 할 수 있으므로 qualitative improvements를 만들어낼 수 있음.

### Image Infilling
- Prompt(Unconditional) : _\<img src="{prefix}\<mask:0\>{postfix}">\<mask:0\>_
- Prompt(Conditional) : _\<img alt="Photo: {text}" src="{prefix}\<mask:0\>{postfix}">\<mask:0\>_ 
  - ![image](https://user-images.githubusercontent.com/18374514/167995649-59dbf992-49fb-4262-8842-6a28ff73610a.png)

### Image Generation
- Prompt(Conditional) : _\<img alt="{prompt}_
  - ![image](https://user-images.githubusercontent.com/18374514/167996090-516f9ecc-2010-4004-897d-7b729e4b2f42.png)
- CLIP 모델을 이용해서 prompt text와 가장 유사하게 생성된 그림을 top4를 뽑음
- 두번째 Prompt에서 빨간색 차를 그리는걸 까먹는다거나 세번째 Prompt에서 양의 얼굴을 그리는 걸 못하는 등의 결함이 보임
- **학습 데이터인 WIki나 News에 등장하지 않는 그림을 그리는 능력은 DALL-E 대비해서 떨어짐**

### Captioning
- Prompt(Masked) : _\<img alt="Photo: A photo taken of \<mask:0\>" src="{image}">_
  - `얘는 왜 맨 뒤에 \<mask:0\> 를 안붙이지?`
- Prompt(Causal) : _\<img src="{image}" title="Photo: A photo taken of_
- CM-3-Caption-CLIP 은 CLIP을 통해서 generation 된 caption 중 top 1을 뽑음
- ![image](https://user-images.githubusercontent.com/18374514/167998056-dd62c221-d15a-4153-b169-ac235ae3680c.png)
- train station이나 red bus와 같은 중요 정보를 놓치는 경우도 있음

### Entitiy Disambiguation
- ![image](https://user-images.githubusercontent.com/18374514/167999245-ba646303-28fb-4071-b610-cf1b597fcb64.png)
- ![image](https://user-images.githubusercontent.com/18374514/168002252-8731727d-c43d-4ca4-a1e0-8fdd062ad34d.png)

### Summarization
- HTLM과 동일한 방식으로 Zero-shot Summarization 가능(Infilling _title_ or _meta_ tag)
- ![image](https://user-images.githubusercontent.com/18374514/168002714-4fec6b54-0645-41ce-9290-38ba6effb527.png)

### Fine-tuning
- GLUE Task Fine-tuning 결과
- ![image](https://user-images.githubusercontent.com/18374514/168002942-384ece1b-8b27-4505-96a8-db8f8e28118f.png)
- T5와 견줄만한 결과
 
# Conlusion
- CM3, multi-modal, trained by html text, new objective(casually masked)
- zero-shot image generation, image captioning, text summarization, entitiy linking 등에 적용 가능
- fine-tuning 도 가능
 
 
# Reference
- [1] https://arxiv.org/pdf/2107.06955.pdf
- [2] https://www.youtube.com/watch?v=GcbT0IGt0xE
