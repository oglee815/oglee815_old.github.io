---
layout: post
date: 2020-09-29 14:00
title: Proximal Policy Optimization Algorithms
author: John Schulman et.al
description: Proximal Policy Optimization Algorithms
comments: true
math: true
category: 
- Reinforcement Learning
tags:
- PPO
- Policy Gradient
- OpenAI
- Paper
---

RL 알고리즘 중 쉬우면서도 성능이 괜찮게 나오는 알고리즘인 PPO 논문 정리자료

 <!--more-->

## Abstract
> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.

## Introduction

- 대표적인 RL Approaches는 다음과 같은것들이 있다.
  - Deep Q-learning 계열(DQN,DDQN,DDDQN..)
  - "Vanila" Policy Gradient(REINFORCE)
  - Trust Region/Natural Policy Gradient
- 이러한 알고리즘들은 아직 Scalable, data efficient, robust 와 같은 측면에서 성능 향상의 가능성이 존재한다.
- Q-Learning은 Discrete Action space 환경에서만 어느정도 성능이 증명됨.
- vanila policy gradient는 poor data effiency and robustness
    > REINFORCE의 경우, 샘플을 한번 업데이트 하면 버려야함, 에피소드가 끝나야 업데이트 가능, 한번 잘못된 정책을 학습하면 돌이킬 수 없는 구렁텅이에 빠지는 등의 단점이 있다고 알고 있음.
- TRPO는 
  - 상대적으로 구현이 어렵고 
  - 아키텍처에 noise(dropout)나 parameter sharing(between the policy and value function, or with auxiliary tasks)를 추가하기 어렵다.
- 여기서 제시하는 PPO는 아래와 같은 특성이 있다.
  - High data efficiency 
  - reliable performance of TRPO
  - fisrt-order optimization(TRPO는 second-order)
- 이를 위해서, <span class='my_highlight'>clipped probabiltiy ratios</span>를 사용함. (lower bound)
- <span class='my_highlight'>performing several epochs</span> of optimization on the sampled data.
    > 이래서 data efficiency가 좋다고 말하는 듯.
- 논문에서는 여러가지 Surrogate objective 를 비교함.
  - Clipped Probabiltiy ratios 가 제일 좋았음.
- Continuos Control Tasks 에서, sample complexity 측면에서 A2C보다 좋고 ACER와 비등함.
    > Sample Complexity 가 뭘까? 그냥 computational complexity?

## Background
### Policy Gradient Method

여기서부터는 **'바닥부터 배우는 강화학습(노승은)'** 에서 발췌
- 정책 네트워크를 \\( \pi_{\theta}(s,a) \\) 하자. \\(\theta\\) 는 네트워크 파라미터이다.
- 일반적으로 뉴럴넷을 학습시킨다는 것은 파라미터를 업데이트하는 것이니 그라디언트 어센트, 혹은 디센트를 사용 할 수 있다. 그러기 위해서는 **로스 함수가 필요하다.**
- 근데 **정책의 로스 함수**는 어떻게 정의할까? 일반적으로 로스함수라 함은 모델의 예측값과 실제 정답과의 차이이다.
- 밸류 네트워크를 학습할 때는 정답지를 MC(REINFORCE)나 TD(Q-Learning)로 구할 수 있었다. (직접 Rollout 해서 구했다는 얘기)
- 하지만 정책 네트워크의 정답이라면 최적의 정책이라는 것인데, 그걸 알면 학습을 할 필요가 없다.
- 따라서 정책 네트워크의 성능을 평가하여 값으로 나타낼 수 있는 함수를 찾은 후, 그 함수의 값을 업그레이드 하는 방식으로 훈련을 해야한다.
- 그 <span class='my_highlight'>평가 함수를 \\(\ J(\theta) \\)</span>라 하자.
- 이 친구는 값이 높아져야 하므로, <span class='my_highlight'>Gradient Accent</span>를 해야한다.
- 좋은 정책이라고 한다면, 그 정책대로 행동했을 때 **보상의 합**이 커야한다.
- 하지만 정책이 고정되어도, 환경의 특성 때문에 매번 받는 보상은 다르다. 따라서 **기대값**이 필요하다.
- 따라서 새로운 목적함수를 정의하면 아래와 같다. 우리는 이 값을 최대화 시켜야 한다.
\\[ J(\theta) = E_{\pi_{\theta}}\[\sum_{t}{r_{t}}\] \\]
- 리워드의 Expectation은 사실 가치 함수이다. 그리고 시작하는 상태가 \\(\ s_{0} \\)로 항상 고정한다면, \\(\ s_{0} \\)의 가치라고 할 수 있다.
\\[ J(\theta) = E_{\pi_{\theta}}\[\sum_{t}{r_{t}}\] = V_{\pi_{\theta}}(s_{0}) \\]
- 만약 시작 위치가 고정되어 있지 않는다면, 어떤 상태 s에서 시작할 확률분포 **d(s)**가 필요하다. (여기서 부터 Policy Gradient Theroem의 시작 같은데.. 저자는 언급을 안함.)
\\[ J(\theta) = \sum_{s\in S}d(s)*V_{\pi_{\theta}}(s)\\]
- 이제, 이 함수를 업데이트 함으로써 정책을 업그레이드 할 수 있다. \\( \nabla_{\theta}J(\theta) \\) 이것을 구해서 아래 식처럼 업데이트 할 수 있다.
\\[ \theta' ← \theta + \alpha * \nabla_{\theta}J(\theta) \\]
- 위를 실행하면 당연히, \\( J(\theta') > J(\theta) \\)이어야 한다. <span class='my_highlight'>따라서 우리의 목적은 \\(\nabla_{\theta}J(\theta)\\)를 찾는게 된다.</span>
- V를 Q에 대한 함수로 다시 써보면 아래와 같다.
\\[ J(\theta) = \sum_{s\in S}d(s)*V_{\pi_{\theta}}(s) = \sum_{s\in S}d(s) * \sum_{a \in A}\pi_{\theta}(s,a) * Q_{\pi_{\theta}}(s,a) \\]
- 이제 양변에 그라디언트를 취해서 값을 구하기만 하면 된다.
\\[ \nabla_{\theta}J(\theta) = \nabla_{\theta}\sum_{s\in S}d(s) * \sum_{a \in A}\pi_{\theta}(s,a) * Q_{\pi_{\theta}}(s,a) \\]
- 여기서,  <span class='my_highlight'>Policy Gradient Theorem </span>에 의해 아래의 식이 성립한다. 미분 텀에 d(s)가 빠진 것이다. (이게 왜 대단한건지는 잘 이해가 안됨. <ins>[Ref](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#a3c))</ins>
\\[ \nabla_{\theta}J(\theta) = \sum_{s\in S}d(s) * \sum_{a \in A}\nabla_{\theta}\pi_{\theta}(s,a) * Q_{\pi_{\theta}}(s,a) \\]
- 하지만 당연히 우리는 Q 값을 알지 못한다. 그래서 이 값을 **"샘플 기반 방법론"**으로 풀 수 있도록 식을 바꿔야 한다. 즉, 식에다가 **기대값**을 취해도 성립이 되게끔 식을 바꿔보자.
\begin{aligned} 
\nabla_{\theta}J(\theta) & = \sum_{s\in S}d(s) * \sum_{a \in A} \nabla_{\theta}\pi_{\theta}(s,a) * Q_{\pi_{\theta}}(s,a) 
\\\\ & = 
\sum_{s\in S}d(s) * \sum_{a \in A} \frac{\pi_{\theta}(s,a)}{\pi_{\theta}(s,a)}\nabla_{\theta}\pi_{\theta}(s,a) * Q_{\pi_{\theta}}(s,a) 
\\\\ & =
\sum_{s\in S}d(s) * \sum_{a \in A} \pi_{\theta}(s,a) \frac{\nabla_{\theta}\pi_{\theta}(s,a)}{\pi_{\theta}(s,a)} * Q_{\pi_{\theta}}(s,a)
\\\\ & =
\sum_{s\in S}d(s) * \sum_{a \in A} \pi_{\theta}(s,a) \nabla_{\theta} \; log\pi_{\theta}(s,a) * Q_{\pi_{\theta}}(s,a)
\\\\ & = 
E_{\pi_{\theta}}[\nabla_{\theta}\;log\pi_{\theta}(s,a) * Q_{\pi_{\theta}}(s,a)]
\end{aligned}
- 기대값 연산자 덕분에 "샘플 기반 방법론"을 이용해 계산을 할 수 있다. \\( \nabla_{\theta}\;log\pi_{\theta}(s,a) \\)는 우리가 정한 뉴럴넷의 그라디언트이기 때문에 쉽게 계산할 수 있다. Q 값은 행동을 취한 뒤 얻는 보상을 관측하면 된다. 이 값을 아주 여러 번 모아서 평균을 내면, 그 평균이 곧 \\( \nabla_{\theta}J(\theta)\\)이다.
- 논문에서는 가장 일반적인 gradient estimator 라면서 아래의 식을 제시.
<span class='centered'>![advantage_pg](/assets/img/advantag_pg.png)</span>
  - \\( \hat{A}\\) = an estimator of the advantage function at timestep t.
  - \\( \hat{E}\\) = empirical average over a finite batch of samples
- 여기서 일반적으로 코드 구현을 할 때, Loss Function을 구한다음, **loss.backward()**를 call 을 함으로써 학습이 된다. 위의 식은 로스가 아니라 그라디언트이므로, <span class='my_highlight'>미분했을 때 위 식이 나오는 식을 찾아야 하고 그 식이 Loss Function</span>이라고 할 수 있다. 그 식이 아래 식이다.
<span class='centered'>![loss_of_pg](/assets/img/loss_of_pg.png)</span>
- 위 식을 여러번에 걸쳐서 업데이트 하면, **empirically** 하게 large policy update를 불러와서 policy를 망칠 수 있다.

### Trust Region Method
- TRPO에서는 아래 surrogate objective function을 constraint를 두고 최대화 하였다.
<span class='centered'>![trpo1](/assets/img/trpo1.png)</span>
  - 위 식은 **conjugate gradient** 알고리즘, 즉 먼저 목적함수를 선형근사 한 다음, constraint 에 대해 quadratic 근사를 함으로써 approximately solved 된다.
- 아래 식도 있는데, 아래 식의 경우 모든 상황에서 잘 동작하는 \\( \beta \\)를 찾기 힘들 뿐더러, 한가지 문제에서도 항상 잘 동작하는 값을 찾기가 힘들기 때문에 위의 Hard Constraint 방식을 사용했다.
<span class='centered'>![trpo2](/assets/img/trpo2.png)</span>

## Clipped Surrogate Objective
- 아래는 논문에서 제시한 Clipped Surrogate Objective
<span class='centered'>![l_clip](/assets/img/l_clip.png)</span>
<span class='centered'>![r_theta](/assets/img/r_theta.png)</span>
- \\( \epsilon \\) 은 hyperparameter로서 continuous control에서는 0.2일 때 성능이 가장 좋았으며, Atari Game에서는 0.1 x α 값을 사용합니다. (여기서 α 는 학습률로 1 에서 시작하여 학습이 진행됨에 따라 0 으로 감소합니다.)  <ins>[Ref](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)</ins>
- min 안에 있는 첫번째 텀은 TRPO에서 썼던 \\( L^{CPI} \\)와 같다.
- min 안에 있는 두번째 텀은 \\(r_{t}\\)값을 1-e, 1+e 사이에서 잡아준다.
- 최종적으로 clipped와 unclipped 값 중 작은 값을 취해주게 되므로, 최종 목적함수는 unclipped objective의 lower bound가 된다.
<span class='centered'>![clip_img](/assets/img/clip_img.png)</span>
- 위 그림은, single timestep 에서의 surrogate function \\( L^{CLIP} \\)을 보여준다. 빨간 점은 starting point이다(r=1). 최종적으로 \\( L^{CLIP} \\)은 이 term들의 합이다.
  - <span class='my_highlight'>내 생각에 그래프를 theta의 업데이트 방향과 양을 파악하는 용도로 봐도 될 것 같음.</span>
  - 아래는 <ins>[여기서](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)</ins> 퍼온글 
  - Advantage Function \\(A^t\\)가 양수일 때
    - Advantage가 현재보다 높다라는 뜻이며 파라미터를 +의 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 s에서 행동 a가 평균보다 좋다는 의미입니다. 따라서 이를 취할 확률이 증가하게 되고, rt(θ)를 clip하여 ϵ보다 커지지 않도록 유도하는 것입니다.
    - 추가적으로, TRPO에서 다뤘던 constraint가 아니라 단순히 clip하는 것이기 때문에 실제로 πθ(at\|st)의 증가량이 ϵ보다 더 커질 수도 있습니다. 하지만 증가량이 더 커졌다고 하더라도 objective function을 업데이트 할 때 효과적이지 않을 수 있기 때문에 대부분 clip을 통해서 ϵ 이하로 유지합니다.
    - 또한 만약 rt(θ)가 objective function의 값을 감소시키는 방향으로 움직이는 경우더라도 1−ϵ보다 작아져도 됩니다. 여기서의 목적은 최대한 lowerbound를 구하는 것이 목적이기 때문입니다. 그러니까 쉽게 말해서 왼쪽 그림에서도 볼 수 있듯이 1+ϵ만 구하는 데에 포커스를 맞추고 있고, 1−ϵ은 신경쓰지 않고 있는 것입니다. (이 부분은 Advantage Function \\(A^t\\)가 음수일 때도 동일합니다.)
    > 만약 A는 양수지만 π 가 삐꾸가 되서, πθ 를 감소시키도록(r < 1-ϵ)로 나온다면, 이 클립 함수는 클립핑을 못한다. \\
    예를 들어, A=3, rθ=0.1, ϵ=0.2 이라면 L = min (0.1 * 3, 0.8 * 3)=0.3 이 된다. 클립핑이 작동했다면 0.8 *3 이 되야하는데 아닌 것이다.
    A가 음수인경우에도, A=-3, rθ=2, ϵ=0.2라면 L = min (2 * -3, 1.2 * -3)= -6으로 클립핑이 안되는 것을 볼 수 있다.
  - Advantage Function \\(A^t\\)가 음수일 때
    - Advantage가 현재보다 좋지 않다라는 뜻이며 그의 반대 방향으로 업데이트 하여야 합니다. 다시 말해 어떠한 상태 s에서 행동 a가 평균보다 좋지 않다는 의미입니다. 따라서 이를 취할 확률이 감소하게 되고, rt(θ)를 clip하여 ϵ보다 작아지지 않도록 유도하는 것입니다.
    > 평균보다 좋지않다 -> Q(S,A) 가 V(S)보다 구리다.
    - 또한 rt(θ)은 확률을 뜻하는 두 개의 함수를 분자 분모로 가지고 있으며, 분수로 구성되어 있기 때문에 무조건 양수로 이루어져 있습니다. Advantage function인 \\(A^t\\)와 곱해져 Objective function인 LCLIP은 Advantage function과 방향이 같아집니다.

<span class='centered'>![linear_interpolation](/assets/img/linear_interporlation_ppo.png)</span>
- 아래는 <ins>[여기서](https://reinforcement-learning-kr.github.io/2018/06/22/7_ppo/)</ins> 퍼온글 
  - LCLIP 은 min 값들의 평균이기 때문에 평균들의 min 값보다는 더 작아집니다. 즉, 주황색 그래프와 초록색 그래프 중 작은값보다 더 작아지는 것을 볼 수 있습니다.
    > LCLIP이 LCPI의 Lower Bound라는건 이해가 된다. 
  - LCLIP을 최대화하는 θ가 다음 πθ가 됩니다. 다시 말해 PPO는 기존 PG 방법들처럼 parameter space에서 parameter θ를 점진적으로 업데이트하는 것이 아니라, 매번 policy를 maximize하는 방향으로 업데이트하는 것으로 볼 수 있는 것입니다.
    > 잘 이해가 되지 않는다. Policy Space에서 Maximize 한다는게 무슨 말이지?\\
    > LCLIP Term 에는 페널티가 없는데 왜 빨간색 그래프가 갈수록 꺾이는지 이해가 되지 않는다.(선형 보간법으로 표현한 그림이라서 그런가?)

## Adaptive KL Penalty Coefficient
논문에서는 KL Divergence를 이용한 또 다른 Objective를 제시한다. Cliped 버전보다 실험 결과가 좋지는 않지만 baseline으로 소개하고 있음.
<span class='centered'>![KLPEN](/assets/img/KLPEN.png)</span>
- 1.5, 2는 휴리스틱하게 정한 값이고 \\(\beta\\)는 또다른 하이퍼파라미터지만 쉽게 조정된다고 함. d_target 또한 하이퍼 파라미터이다.

## Algorithm
알고리즘은 LPG 대신 LCLIP 혹은 LKLPEN을 사용해서 multiple step(epoch?)만큼 업데이트 해주면 끝이다. 
- variance-reduced advantage-function estimators로는 주로 learned state-value function V(s), 예를 들어 GAE, finite-horizon estimator를 사용한다.
- Policy와 Value를 파라미터 쉐어링 한다면(같은 네트웍을 쓴다면?) loss function 에 surrogate policy와 value function error을 합쳐야한다. 이 때 추가적으로 entropy bonus를 추가해서 exploration을 더 할 수 있도록 만든다.
  > 노승은님 코드에서는 pi와 v 가 같은 네트웍을 쓰고 있으므로 이 로스를 사용해야 함.

<span class='centered'>![loss_augmented](/assets/img/loss_augmented.png)</span>

- <ins>[A3C 논문](https://arxiv.org/pdf/1602.01783.pdf)</ins>에서 사용되었고 RNN에 잘 맞는 방법은 T timesteps 만큼(episode보다 훨씬 작은) 샘플을 모으고 업데이트 하는 것이다. 이 방식은 advantage estimator가 필요하다.(does not look beyond timestep T) 아래 식 사용.
  - <span class='centered'>![MNI_estimator](/assets/img/MNI_estimator.png)</span>
- 여기서는 truncated version of generalized advantage estimation을 사용가능한데, λ = 1로 두면, 아래 식이 위와 같다. (근데 왜 람다를 그냥 적어두는거지?)
  - <span class='centered'>![gae](/assets/img/gae.png)</span>
- 위 식에서 δt 는 사실 At 랑 똑같다.
  > 내가 아는 A는 q(s,a) - v(s) 인데, Q(s,a) = rt + gamma * V(st+1) 로 한 뒤에 rt + gamma * V(st+1) -V(s)로 써도 되는건가?
- 결과적으로, 아래 알고리즘으로 돌아간다.
  - <span class='centered'>![algorithm1_ppo](/assets/img/algorithm1_ppo.png)</span>
- 논문에서는 위의 알고리즘만 수도코드로 나와있지만 이 <ins>[강의](https://www.youtube.com/watch?v=xvRrgxcpaHY)</ins>에서는 아래 두 알고리즘에 대한 수도코드도 있다.
  - <span class='centered'>![algorithm2_ppo](/assets/img/algorithm2_ppo.png)</span>
  - <span class='centered'>![algorithm3_ppo](/assets/img/algorithm3_ppo.png)</span>

## Experiments
- 아래 3개의 objective 사용
  - <span class='centered'>![3ppo_objective](/assets/img/3ppo_objective.png)</span>
  - 7 Mujoko, 1M steps 동안 훈련.
  - Fully-Connected MLP with two hidden layers of 64 units, tanh nonlinearities, outputting gaussian distribution with standard deviations
  - Hyperparams
    - <span class='centered'>![ppo_hyperparams](/assets/img/ppo_hyperparams.png)</span>
- 다른 알고리즘과의 비교(그래프 생략)
  - 아래는 노승은님 코드로 내가 직접 cartpole 돌려본 결과.
  - 500점을 10번 찍은 에피소드를 적었음.
  - 확실히 PPO의 Epoch을 늘리면 빠르고 안정적으로 미션을 클리어하는 것 같음.

|알고리즘|0|	1|	2|	avg|	std|
|----|----|----|----|----|----|
|ppo_eopch1		|483|	500|	541|	508|	24.344746|
|ppo_eopch3		|133	|278	|306	|239	|75.819962|
|ppo_eopch5		|206	|320	|428	|318	|90.642154|
|ppo_eopch8		|121	|185	|196	|167	|33.069372|
|ppo_eopch10		|143	|157	|163	|154	|8.381527|
|ppo_eopch20		|168	|221	|232	|207	|27.940413|
|----|----|----|----|----|----|
|Actor Critic|	414	|517|	584|	505|	69.919001|
|REINFORCE		|568	|630	|650	|616	|34.909407|
|DQN				|632|	677	|800	|703	|71.007042|

 - 마지막은 ACER와의 비교인데, PPO가 마지막에 가서는 ACER보다 딸리지만 전체적으로 점수가 높은 경우는 더 많은걸봐서 처음에는 ACER보다 안정적으로 높은 점수를 기록한다는 걸 보여주려는 것 같음.
 - <span class='centered'>![ppo_acer](/assets/img/ppo_acer.png)</span>