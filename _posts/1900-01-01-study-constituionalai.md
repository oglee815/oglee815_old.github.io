---
layout: post
date: 2020-09-17 14:08
created_date: 2020-09-17 14:08
title: "Constituional AI"
description: "Constituional AI"
comments: true
mathjax: true
category:
- Paper
tags:
- ChatGPT
- Anthropic
- Claude
- Constituional AI
---

Constituional AI

<!--more-->

# [Google VS MS](https://www.youtube.com/watch?v=_TAWaueEmoY)
- Anthropic vs OpenAI, Claude vs ChatGPT
- Anthropic
  - Founded in 2021 by former OpenAI execs: Daniela Amodei and Dario Amodei
  - **Claude** is pretty similar to ChatGPT(funtionally)
  - Google invests nearly $400M into Anthropic
- Claude & Constitutional AI
  - This is a basic **heuristic imperative cognitive architecture.**
  - It teaches itself based on internal feedback and guiding principles rahter than RLHF like ChatGPT
  - ![image](https://user-images.githubusercontent.com/18374514/217284423-f4809f20-8559-43b8-b116-5472e1af336a.png)
- OpenAI Philosophy
  - Seems to believe in the "One model to rule them all" paradigm.
  - Basically "Scale is all you need"
  - More data, more parameters, more layers
  - RLHF : they don't seem to know anything about epistemology(인식론), cognition, or philosophy
    - Aka "alignment" just means "do what humans want" (which is a bad idea)
- Anthropic Philsophy
  - Constitional AI : Reduce Harm
    - "Harm reduction: is a proven philosophy in public health policy and medicine
    - Aka, goal is "achieve harmlessness"
- RLHF vs Learned harmlessness
  - Pure RLHF is probabbly better from a product perspective, meaning that ChatGPT will be more useful in the short term.
    - It's basically automated Agile
    - Problem : human willpower not always good, and people don't always know what they really want/need.
  - Learnd harmlessness is a good abstract principle
    - "Be helpful while also being harmless"
    - This has limitation
- Pros & Cons of RLHF
  - Cons
    - No principles other than "do what humans want"
    - This is bad, since humans are individually unreliable
    - OpenAI already having to fight this manually
  - Pros
    - Easy to implement
    - Gets good results up front
  - ![image](https://user-images.githubusercontent.com/18374514/217289910-74787c15-23b6-41d4-8612-79defbde7d14.png)
- Pros & Cons of CAI(Learned Harmlessness)
  - Cons
    - Peak Harmlessness == do nothing
    - Needs other goals to offset neutrailty(중립)/inertness(불활성)
    - Less responsive to user desires
  - Pros:
    - Far better for humanity in the long run
    - More trustworthy (actually uses a moral framework)
    - ![image](https://user-images.githubusercontent.com/18374514/217290658-de97f841-41b3-48c0-bae3-aa6474be7ee3.png)
- ![image](https://user-images.githubusercontent.com/18374514/217291094-43866578-85c3-4004-9b34-bb6975400741.png)
- ![image](https://user-images.githubusercontent.com/18374514/217291235-21de846f-f103-4e74-86b7-4e5d2d69c375.png)
- ![image](https://user-images.githubusercontent.com/18374514/217291903-65b2e771-ccb8-4a4e-bf57-9935edf4f507.png)
