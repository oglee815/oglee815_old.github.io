---
layout: post
date: 2020-09-11 11:42
title: PowerBERT
description: Accelerating BERT Inference via Progressive Word-vector Elimination
comments: false
category: 
- NLP
tags:
- NLP
- Distilation
---

## ì–´ë–¤ ë‚´ìš©ì˜ ë…¼ë¬¸ì¸ê°€ìš”? ğŸ‘‹

ìƒëŒ€ì ìœ¼ë¡œ ëœ ì¤‘ìš”í•œ Word vectorë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨, GLUE Task ì—ì„œ BERT ëŒ€ë¹„ Accì˜ ì†ì‹¤ì€ 1% ì´í•˜ë¡œ ìœ ì§€í•˜ë©´ì„œ ì†ë„ëŠ” ìµœëŒ€ 4.5ë°° í–¥ìƒí•¨.

<!--more-->

## Abstract (ìš”ì•½) ğŸ•µğŸ»â€â™‚ï¸

> We develop a novel method, called PoWER-BERT, for improving the inference time of the popular BERT model, while maintaining the accuracy. It works by: <strong>a) exploiting redundancy pertaining to word-vectors (intermediate encoder outputs) and eliminating the redundant vectors. b) determining which word-vectors to eliminate by developing a strategy for measuring their signiï¬cance, based on the self-attention mechanism. c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function.</strong> Experiments on the standard GLUE benchmark shows that PoWER-BERT achieves up to 4.5x reductionin inference time over BERT with < 1% loss in accuracy. <strong>We show that PoWER-BERT offers signiï¬cantly better trade-off between accuracy and inference time compared to prior methods</strong>. We demonstrate that our method attains up to 6.8x reduction in inference time with < 1% loss in accuracy when applied over ALBERT, a highly compressed version of BERT. The code for PoWER-BERT is publicly available at https://github.com/IBM/PoWER-BERT.


## ì´ ë…¼ë¬¸ì„ ì½ì–´ì„œ ë¬´ì—‡ì„ ë°°ìš¸ ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”! ğŸ¤”

### 1. Introduction

#### Motivation

- BERT ê°€ NLP ë¶„ì•¼ì—ì„œ significant success í•˜ë‹¤ëŠ” ê±´ ë‘ë§í•˜ë©´ ì…ì•„í””
- í•˜ì§€ë§Œ ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì„±ëŠ¥ ë§Œí¼ì´ë‚˜ resource ì™€ latency ë¬¸ì œë„ ì¤‘ìš”í•¨.
- ë”°ë¼ì„œ BERTì˜ computational demandsë¥¼ ì¤„ì´ê¸° ìœ„í•œ ì—°êµ¬ë¥¼ ì§„í–‰í•¨

#### Previous Work
- **ALBERT** : Parameter Sharing ê³¼ Decomposing Embedding Layer ë°©ë²•ìœ¼ë¡œ Model Sizeë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì˜€ìœ¼ë‚˜, Inference Timeì—ì„œ ì´ë“ì€ ì—†ìŒ(ê³„ì‚°ëŸ‰ ìì²´ëŠ” ì¤„ì´ì§€ ëª»í–ˆìœ¼ë¯€ë¡œ).
- **Knowledge Distillation (DistilBERT, BERT-PKD)** ì´ë‚˜ , **Encoder/Head-Prune** ë°©ì‹ì€ Accuracy ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì‹¬ê°í•œ ì €í•˜ë¥¼ ê°€ì ¸ì˜´.

#### Our Objective and Approach
- Object : AccuracyëŠ” ì–´ëŠì •ë„ ìœ ì§€í•˜ë©´ì„œ Inference Timeì€ ëŒ€í­ ì¤„ì´ëŠ” ê²ƒì´ ëª©ì 
- Approach : **ëª¨ë¸ ì‚¬ì´ì¦ˆëŠ” ìœ ì§€. ëŒ€ì‹  Encoder ë¥¼ ê±°ì¹˜ëŠ” ë™ì•ˆ ìƒì„±ë˜ëŠ” Word Vectorì¤‘ ì¼ë¶€ë¥¼ ì œê±°í•¨.**
- Main Contributions
    - ë¶ˆí•„ìš”í•œ Word Vectorë¥¼ ì œê±°í•˜ëŠ” Scheme ì¸ **PoWER-BERT ì œì•ˆ(Progressive Word-vector Elimination for inference time Reduction of BERT)**
    - GLUE Tasks ì—ì„œ BERTë³´ë‹¤ 4.5x ë¹ ë¥´ê³  ì„±ëŠ¥ í•˜ë½ë„ 1% ì´í•˜ë¡œ ìœ ì§€
    - ë‹¤ë¥¸ Reduction Methods ì™€ Speed & Acc ì‚¬ì´ì˜ Trade off ë¹„êµ
    - ALBERTì—ë„ ì ìš©í•˜ë©´ 6.8x ë¹¨ë¼ì§.
- ê¸°íƒ€ ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰í•œ Reduction Methods : 
    - pruning network connections, pruning filters/channels from CNN, weight quantization, Knowledge Distillation, singular value decomposition of weight matrices
- ê°•ì¡°í•˜ëŠ” íŠ¹ì§• : PoWER-BERTëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìì²´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜, word-vectorsë¥¼ ì œê±°í•˜ëŠ” ë°©ì‹ì´ë¯€ë¡œ ê¸°ì¡´ì˜ BERT, ALBERT ë“±ì˜ êµ¬ì¡°ì— ë°”ë¡œ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì¥ì ì´ ìˆìŒ.

### 2. Background

- BERT Base size : 12 encoders, 12 self-attention heads, hidden size 768
- [BERT architecture](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/bert-encoder?fbclid=IwAR0YUclKFzwx-2MEqRb_X_yTePqFju2E_oLHbcmrnWTmTIxoYe5gnRgH6CE)
- Self-Attention : 
![self-attention](https://user-images.githubusercontent.com/18374514/90849067-5f88cb00-e3a9-11ea-9cdf-a8233a69159e.png)
![self-attention2](https://user-images.githubusercontent.com/18374514/90849828-2a7d7800-e3ab-11ea-981e-7aeee72fa8a9.png)
M : N X 768 input, W<sub>q</sub><sup>h</sup>, W<sub>k</sub><sup>h</sup>, W<sub>v</sub><sup>h</sup> : query, key, value matrix


### 3. PoWER-BERT Scheme

#### Motivation
- BERTë¡œ Classification ë¬¸ì œë¥¼ í•´ê²° í•  ë•Œ, ì‚¬ìš©ë˜ëŠ” í† í°ì€ ***[CLS]*** ë¿ì„.
- ê·¸ëŸ¬ë‚˜ CLS ì™¸ì— ë‹¤ë¥¸ ìœ„ì¹˜ì˜ í† í°ì„ Classificaiotn ì— ì‚¬ìš©í–ˆì„ ë•Œë„ í¬ê²Œ Acc ê°€ ë‹¬ë¼ì§€ì§€ ì•Šì•˜ìŒ. (í‰ê· ì ìœ¼ë¡œ 1.2% ì°¨ì´)
- ë”°ë¼ì„œ, Word Vector ê°„ì—ëŠ” **diffusion(í©ì–´ì§) of information** ì´ ì¡´ì¬í•¨ì„ ë°œê²¬í•¨.

#### Diffusion of Information
- Word Vector ê°€ Encoder ë¥¼ í†µê³¼í•˜ë©´ì„œ ì ì°¨ì ìœ¼ë¡œ ë¹„ìŠ·í•œ ì •ë³´ë¥¼ ë‹´ê²Œ ë¨(due to the self-attention)
![cosine_similarity](https://user-images.githubusercontent.com/18374514/90853529-6289b880-e3b5-11ea-9d89-d4b9bdc00677.png)
SST-2 ë°ì´í„° ì…‹ì— ëŒ€í•œ Word Vector ê°„ Cosine Similarity. ë§¤ ë ˆì´ì–´ë³„ë¡œ <sub>128</sub>C<sub>2</sub>ì˜ ëª¨ë“  ì¡°í•©ì— ëŒ€í•œ Average ê°’.
- ìœ„ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼, Word-Vector ë¼ë¦¬ ë¹„ìŠ·í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆê²Œ ë˜ë¯€ë¡œ, ***[CLS]*** ì™¸ì— ë‹¤ë¥¸ Vectorë¥¼ ì‚¬ìš©í•´ë„ ì„±ëŠ¥ì´ í° ì°¨ì´ê°€ ì—†ìŒ.
- PoWER-BERTì—ì„œëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ì„œë§Œ Word Vector ì¤‘ ì¼ë¶€ë¥¼ ì œê±°í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, **ì „ì²´ Encoder Pipeline ì— ë”°ë¼ì„œ ì ì°¨ì ìœ¼ë¡œ ì œê±°í•˜ëŠ” ê²ƒì´ í•µì‹¬**

#### PoWER-BERT Components
- 2ê°€ì§€ Components ê°€ ì¡´ì¬.
    - a retention configuration : ê° ë ˆì´ì–´ ë³„ë¡œ ëª‡ ê°œì˜ Word Vectorë§Œ ê°€ì ¸ê°ˆì§€ì— ëŒ€í•œ Configuration. ì•„ë˜ ê·¸ë¦¼ì˜ ê²½ìš°ëŠ” 
(80,73,70,50,50,40,33,27,20,15,13,3)
![retention](https://user-images.githubusercontent.com/18374514/90855147-cf06b680-e3b9-11ea-9418-58e975d0dddd.png)
    - Word Vector Selection : ì–´ë–¤ word-vectorsë¥¼ ê°€ì ¸ê°€ì•¼ í•˜ë‚˜?

#### Word-vector Selection
- Static and Dynamic Strategies
    - Static : ë§¤ ë ˆì´ì–´ ë³„ë¡œ ì§€ì›Œì•¼ í•  ê°œìˆ˜ì™€ ìœ„ì¹˜ë¥¼ ê³ ì •í•¨. 
        - Head-WS : ìˆœì„œëŒ€ë¡œ ì•ì— Jê°œë§Œ ì„ íƒ
        - Rand-WS : ëœë¤í•˜ê²Œ Jê°œë§Œ ì„ íƒ
    - Dynamic : ë ˆì´ì–´ë‚˜ Dataset ì— ë”°ë¼ì„œ ë‹¤ë¥´ê²Œ ì„ íƒí•¨.
        - Attn-WS : Attention Mechanism ì„ ì´ìš©í•´ì„œ 

- Attention-based Scoring
![significant_score](https://user-images.githubusercontent.com/18374514/90867090-cfaa4780-e3cf-11ea-8940-3dcbbc8bc1da.png)

- Word-vector Extraction 
    - self-attention ê³¼ feed-forward network ì‚¬ì´ì— extract layerë¥¼ ì¶”ê°€í•˜ì—¬ Score ê¸°ì¤€ Top Lê°œë¥¼ ì„ íƒí•¨. ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” 4ê°œ, 2ê°œ ì„ íƒ.
![extract_layer](https://user-images.githubusercontent.com/18374514/90868238-8a871500-e3d1-11ea-9e8b-f1265841dc75.png)

- Validation of the Scoring Function
    - Scoring function ì˜ ìœ íš¨ì„± ê²€ì¦ì„ ìœ„í•´ **Mutual Information(MI)** ì´ìš©í•˜ì—¬ ì‹¤í—˜ ì§„í–‰.
        - X : BERTì˜ ì•„ì›ƒí’‹, Y : PoWER-BERTì˜ ì•„ì›ƒí’‹. MI(X|Y) ëŠ” ë†’ì„ ìˆ˜ë¡ ì¢‹ìŒ.
    - ì‹¤í—˜ ê²°ê³¼, Score ê°€ ë†’ì€ ë‹¨ì–´ë¥¼ ì œê±° í•  ìˆ˜ë¡ MIê°€ ê°ì†Œí•¨.
    - ì¦‰, Score ì™€ final classification ì— ëŒ€í•œ ì˜í–¥ë„ëŠ” ì–‘ì˜ ìƒê´€ê´€ê³„ì— ìˆë‹¤.
![mutual](https://user-images.githubusercontent.com/18374514/90885415-84eaf880-e3ec-11ea-94d2-6aafb6219d1f.png)
    - Kê°€ ì¦ê°€í• ìˆ˜ë¡(ìŠ¤ì½”ì–´ê°€ ë†’ì•„ì§ˆìˆ˜ë¡), MIëŠ” ì»¤ì§„ë‹¤. ì¦‰, Scoreê°€ ë†’ì€ Wordë¥¼ ì œê±°í•˜ë©´ MIê°€ ê°ì†Œí•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤. ë°˜ëŒ€ë¡œ Scoreê°€ ë‚®ì€ Wordë¥¼ ì œê±°í•˜ë©´ MIëŠ” ì¦ê°€. (K : jë²ˆì§¸ ì¸ì½”ë”ì˜ word vectorë“¤ ì¤‘, Scoreê°€ ë†’ì€ ìˆœìœ„)
    - Wordê°€ ì œê±°ë˜ëŠ” Encoder ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ MIê°€ ì¦ê°€ -> Scoreê°€ ë†’ì€ ë‹¨ì–´ëŠ” ë” ê¹Šì€ ë ˆì–´ì—ì„œ ì œê±°ë˜ì•¼ í•œë‹¤. ì²˜ìŒë¶€í„° ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ì œê±°í•´ë²„ë¦¬ë©´ MIê°€ ë‚®ì•„ì§.

#### Retention configuration
- ê° ë ˆì´ì–´ ë³„ë¡œ ëª‡ ê°œì˜ word vectorë¥¼ ë‚¨ê²¨ì•¼ í•˜ëŠ”ê°€? --> Scoreê¸°ì¤€ìœ¼ë¡œ sorting í•˜ì—¬ ê²°ì •
- Soft-extract layer : extract layerëŠ” Scoreë¥¼ ê¸°ì¤€ìœ¼ë¡œ word-vectorë¥¼ ë‚¨ê¸¸ì§€ ë²„ë¦´ì§€ ê²°ì •í•˜ì§€ë§Œ, soft-extractëŠ” ëª¨ë“  vectorë¥¼ ë‚¨ê¸°ë˜ ê° scoreì— ë”°ë¼ì„œ ë‚¨ê¸°ëŠ” ì •ë„ë¥¼ ì¡°ì ˆ.
- r<sub>j</sub>[N] ëŠ” 0-1ì‚¬ì´ ê°’ì„ ê°–ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°(retention parameter). í•´ë‹¹ ìœ„ì¹˜ì˜ vectorë¥¼ ì–¼ë§ˆë§Œí¼ ë‚¨ê¸¸ì§€ ê²°ì •.
![soft-extractor](https://user-images.githubusercontent.com/18374514/90894715-0007db00-e3fc-11ea-8cbd-715a90a16909.png)

#### Loss Function
- mass : jë²ˆì§¸ Encoderì˜ score ìˆœìœ¼ë¡œ sorting ëœ vectorë“¤ì— ì ìš©ë˜ëŠ” r ê°’ì˜ í•© -> ì´ê±¸ ìµœì†Œí™” í•˜ë„ë¡ Loss êµ¬í˜„
- theta : BERT ì˜ íŒŒë¼ë¯¸í„°, L : Cross entropy Loss, lambda : hyper-param
- r ê°’ì€ 1ë¡œ ì´ˆê¸°í™” ë¨-> ì¦‰ ì²˜ìŒì—ëŠ” ëª¨ë“  vector ê°€ ìˆœìˆ˜í•˜ê²Œ ì „ë‹¬ë¨.
![loss](https://user-images.githubusercontent.com/18374514/90907702-45cd9f00-e40e-11ea-8dc4-d5bb26513962.png)
![mass](https://user-images.githubusercontent.com/18374514/90907899-99d88380-e40e-11ea-9c84-b37e3f051a24.png)

#### Training PoWER-BERT
ì´ 3ê°œì˜ ìŠ¤í…ìœ¼ë¡œ ì§„í–‰
1) ***Fine-tunning*** : Pre-trained BERT ëª¨ë¸ë¡œ Fine-tuning
2) ***Configuration-search*** : Soft-extract Layerë¥¼ fined-tunned ëª¨ë¸ì— ì¶”ê°€í•˜ê³  Loss functionì„ ìˆ˜ì •. Inference Timeê³¼ acc ì‚¬ì´ì˜ ì ì ˆí•œ trade-offë¥¼ ì°¾ê¸° ìœ„í•´ Lambda ì™€ retention parameter ë¥¼ í•™ìŠµí•¨. ì´ë•Œ LRì€ 10~100ë°°ê¹Œì§€ í¬ê²Œ ì‚¬ìš©.
3) ***Re-training*** : Soft-extract layerë¥¼ extract layerë¡œ ëŒ€ì²´. ì œê±°ë  word-vectorë“¤ì˜ ìˆ«ìëŠ” 2)ì—ì„œ ì°¾ì€ configurationìœ¼ë¡œ ê²°ì •. ì–´ë–¤ word vectorë¥¼ ì œê±°í• ì§€ëŠ” significance scoreë¡œ ê²°ì •. **(CLSëŠ” ì œê±°ë˜ì§€ ì•ŠìŒ)**

### 4. Evaluation

![table2](https://user-images.githubusercontent.com/18374514/90912405-8250c900-e415-11ea-9517-70ad48fb8676.png)
- BERT ëŒ€ë¹„
    - lambdaë¥¼ ì¡°ì ˆí•¨ìœ¼ë¡œì¨ Acc Loss ëŠ” 1% ë°‘ìœ¼ë¡œ ìœ ì§€í•˜ë©´ì„œ Inference timeì—ì„œ 2.0x ~ 4.5x ë¹ ë¦„. 
    - Input Sequence Length ê°€ 256ì¸ Taskì˜ ê²½ìš°, BERTëŠ” 12 X 256 = 3072 ê°œì˜ Word-Vector ì‚¬ìš©. 
    - PoWER-BERTì˜ ê²½ìš°, ë‹¤ìŒì˜ configuration ì‚¬ìš©í•˜ë©´ (153,125,111,105,85,80,72,48,35,27,22,5) ì´ 868ê°œë§Œ í•„ìš”.
    - Self-Attention ê³¼ feed-forward network ëª¨ë“ˆì˜ ì—°ì‚°ëŸ‰ì€ Word Vectorê°œìˆ˜ì— ë¹„ë¡€í•˜ë¯€ë¡œ, ì†ë„ê°€ ë¹¨ë¼ì§,

- ë‹¤ë¥¸ Methods ì™€ ë¹„êµ
![method_comparision](https://user-images.githubusercontent.com/18374514/90913718-9e556a00-e417-11ea-9a61-863045c65985.png)

- ALBERTì— ì ìš©
    - ë§ˆì°¬ê°€ì§€ë¡œ Acc LossëŠ” 1%ë¯¸ë§Œìœ¼ë¡œ ìœ ì§€í•˜ê³  ì†ë„ëŠ” 2~6.8x ë¹¨ë¼ì§.
> ì™œ QQPì—ì„œ ìœ ë… ì†ë„ í–¥ìƒì´ í´ê¹Œ? ì¶”ì¸¡ìœ¼ë¡œëŠ” QQP ë°ì´í„°ì…‹ì—ì˜ exampleë“¤ì˜ ëŒ€ë¶€ë¶„ì´ ì§§ì€ ë¬¸ì¥ì´ë¼ì„œ. 
![Table3](https://user-images.githubusercontent.com/18374514/90912492-a7ddd280-e415-11ea-8d0c-a78e24a42ee7.png)

- Ablation Test : Static vs Dynamic Word Selection
![ablation](https://user-images.githubusercontent.com/18374514/90915198-16bd2a80-e41a-11ea-91f6-05200f141b33.png)

- Anecdotal Examples
    - SST-2 (ê°ì •ë¶„ì„) ì˜ˆì œì— ëŒ€í•œ Word Vector Elimination ì˜ˆì œ. Retention Config (7,7,7,7,4,4,4,4,2,2,2,2)
    - ë‘ ì˜ˆì œ ëª¨ë‘ ì²«ë²ˆì§¸ëŠ” stop wordsì™€ punctuationì´ ì œê±°ë¨.
    - ìµœì¢… ë ˆì´ì–´ì—ì„œ ì„ íƒë˜ëŠ” ë‘ ê°œì˜ ë‹¨ì–´ë“¤ë¡œë„ ì¶©ë¶„íˆ ê°ì • ë¶„ì„ì´ ê°€ëŠ¥(Diffusion of information ë•ë¶„)
![anecdotal](https://user-images.githubusercontent.com/18374514/90915467-92b77280-e41a-11ea-8746-f1f4912d6f02.png)

### 5. Conclusion
- PoWER-BERTëŠ” ëœ ì¤‘ìš”í•œ Word vectorë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨ GLUE Task ì—ì„œ BERT ëŒ€ë¹„ Accì˜ ì†ì‹¤ì€ 1% ì´í•˜ë¡œ ìœ ì§€í•˜ë©´ì„œ ì†ë„ëŠ” 4.5ë°° í–¥ìƒí•¨.
- ë‹¤ë¥¸ Methodì— ë¹„í•´ì„œ Accì™€ Speed ê°„ì˜ Trade-offë¥¼ ë§¤ìš° í–¥ìƒì‹œí‚´.
- ë˜í•œ, ALBERTì—ë„ ì ìš©ê°€ëŠ¥.
- ì•ìœ¼ë¡œ ë²ˆì—­ì´ë‚˜ ìš”ì•½ê³¼ ê°™ì€ Taskì—ë„ ì ìš©í•  ì˜ˆì •.

## ê°™ì´ ì½ì–´ë³´ë©´ ì¢‹ì„ ë§Œí•œ ê¸€ì´ë‚˜ ì´ìŠˆê°€ ìˆì„ê¹Œìš”?

ê²½ëŸ‰í™”ë¥¼ ìœ„í•œ ë‹¤ë¥¸ ë°©ë²•ë“¤ : <a>https://blog.est.ai/2020/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8-%EC%95%95%EC%B6%95-%EB%B0%A9%EB%B2%95%EB%A1%A0%EA%B3%BC-bert-%EC%95%95%EC%B6%95/</a>

í•˜ì´í¼ íŒŒë¼ë¯¸í„° : <a>https://proceedings.icml.cc/static/paper_files/icml/2020/6722-Supplemental.pdf</a>

## ë ˆí¼ëŸ°ìŠ¤ì˜ URLì„ ì•Œë ¤ì£¼ì„¸ìš”! ğŸ”—

ë…¼ë¬¸ : <a>https://proceedings.icml.cc/static/paper_files/icml/2020/6722-Paper.pdf</a>
Github : <a>https://github.com/IBM/PoWER-BERT</a>
BERT Architecture : <a>https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/bert-encoder?fbclid=IwAR0YUclKFzwx-2MEqRb_X_yTePqFju2E_oLHbcmrnWTmTIxoYe5gnRgH6CE</a>
